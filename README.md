
## News article clustering using K-means: A comparative analysis between Frequency-Inverse Document Frequency (TF-IDF) and Document to Vector (Doc2Vec)

This study aims to create a framework similar to a news aggregator by clustering thirty news articles 
describing issues the Biden administration faces into similar topics using the K-means algorithm. 
A comparative analysis between vector representation methods Frequency-Inverse Document
Frequency (TF-IDF) and Document to Vector (Doc2Vec) helps identify which method is best 
suited for a small corpus . 


## Documentation

[Code](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/Code.ipynb)

[Data](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/Corpus.csv)


## Methodology

#### Pre-Processing

- Tokenization: splitting each document into individual words using the .split() method.
- Removing punctuation from each word using regular expressions.
- Removing non-alphabetic words using the .isalpha() method.
- Removing words with lengths shorter than four letters.
- Transforming all words into lowercase using the .lower() method.
- Filtering out stop words that offer no semantic value, such as (like, therefore, which).

#### Representation Methods

- #### TF-IDF
    
    Method that converts words into numerical vector representations allowing for further analysis using machine learning methods (Kim et al. 2019, 18). 

    #### TF-IDF formula
    ![TF-IDF Formula](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/tfidf_formula.png)

    This unsupervised weighting method assigns a higher weight to frequent words within a document that are scarce across a corpus. 
    Words with higher weights describe the most relevant words in a corpus (Carvalho and Guedes 2020, 4-5).

    These feature weights serve as parameters for partitioning methods that use distance-based measures to cluster documents based on vector 
    distance, such as the K-means clustering algorithm used in this study (Aggarwal and Reddy 2018, An Introduction 
    to Cluster Analysis).

- #### Doc2Vec

    Neural net approach based on Word2Vec that creates a dense vector for each document in the corpus rather than each word (Chen and Sokolova 2018, 13). 
    
    The vectors generated by Doc2Vec unite similar documents based on their distance in vector space.

- #### Cosine Similarity

    The TF-IDF and Doc2Vec vectorization models require a method to measure the similarity 
    between vectors in the document. 
    
    A standard method is the cosine similarity which measures the 
    cosine of the angle between two vectors to determine the similarity between two documents 
    (Turney and Pantel 2010, 160).

    #### Cosine formula
    ![Cosine Formula](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/cosine_similarity_formula.png)

    Where d1 and d2 represent the weight of the vectors representing the documents in 
    an n-dimensional space (Kedia and Rasu 2020a, Transforming Text into Data Structures).
    
    The cosine similarity value is between -1 and +1, where +1 indicates that the vectors are similar 
    and -1 indicates that the vectors are not. (Kedia and Rasu 2020a, Transforming Text into Data 
    Structures).

#### K-Means Algorithm

Following the document representation phase and the selection of the similarity measure, the 
documents are clustered. 

This study uses the K-means algorithm, a clustering method that works 
by partitioning documents into similar clusters given an initial set of clusters k (Bouras and 
Tsogkas 2012, 116). 

The K-means algorithm works as follows: 

    1. Manually select k clusters as the initial set of centroids, 
    2. Assign document clusters to the closest centroid, 
    3. Calculate a new centroid for each cluster of documents, 
    4. Repeat steps 2 and 3 until no further centroid reassignments are necessary (117).

- #### Choosing K

    The elbow method does not provide a clear number of K's in this study. K=8 is used as a middle ground number.

    ##### Elbow Method TF-IDF
    ![Cosine Formula](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/tfidf_elbow.png)

    ##### Elbow Method Doc2Vec
    ![Cosine Formula](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/doc2vec_elbow.png)

- #### Clustering

    Applying the K-means algorithm to the TF-IDF and Doc2Vec data frames containing the vector features assigned to each document creates clusters of news articles.


## Results

The K-means clustering results plots show that TF-IDF outperforms Doc2Vec in grouping the thirty news articles into semantically similar clusters. 

The short list of news articles' titles also provide a straightforward way of validating the accuracy of the clusters.

#### TF-IDF Evaluation Metrics
- Homogeneity: 1.000
- Completeness: 1.000

#### TF-IDF Plot
![TF-IDF Plot](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/tfidf_cluster_plot.png)

#### Doc2Vec Evaluation Metrics
- Homogeneity: 0.384
- Completeness: 0.390

#### Doc2Vec Plot
![Doc2Vec Plot](https://github.com/patsarmient/patsarmient-py_K-Means-Clustering-News-Articles/blob/main/doc2vec_cluster_plot.png)


## Analysis

#### Why TF-IDF outperforms Doc2Vec

Doc2Vec is a neural network technique that relies on large datasets to improve its ability to predict a word's semantic patterns within a document and, in turn, predict inter-document similarities. 

In contrast, this paper uses thirty news articles, a considerably low number of documents, and 3257 Word2Vec tokens to create weighted vectors.

The TF-IDF vectorization method performs better on small corpora because it relies on the lexical equivalence of words, meaning that the occurrence of words and their match within a corpus is what contributes to the development of associations between documents, not the semantic meaning of words. 


## Acknowledgements

Thank you to paulhuynh for the source code provided as part of the Northwestern Univerity Natural Language Processing course MSDS 453.


## References

Aggarwal, Charu C., and Chandan K. Reddy. 2018. “An Introduction to Cluster Analysis.” In
Data Clustering: Algorithms and Applications. O'Reilly. Boca Raton, FL: Chapman and 
Hall/CRC. https://learning.oreilly.com/library/view/dataclustering/9781466558229/K15510_C001.xhtml#text_Dest-19.

Bouras, Christos, and Vassilis Tsogkas. 2012. “A Clustering Technique for News Articles Using 
Wordnet.” Knowledge-Based Systems 36 (December 2012): 115–28. 
https://doi.org/10.1016/j.knosys.2012.06.015. 

Chen, Qufei, and Marina Sokolova. 2018. “Word2Vec And Doc2vec in Unsupervised Sentiment 
Analysis of Clinical Discharge Summaries.” arXiv.org. 
https://arxiv.org/abs/1805.00352v1. 

Kedia, Aman, and Mayank Rasu. 2020a. “Transforming Text into Data Structures.” In Hands-on 
Python Natural Language Processing. O'Reilly Online Learning. Packt Publishing.
https://learning.oreilly.com/library/view/hands-on-pythonnatural/9781838989590/8b221713-8fb7-4f7e-98dc-4f4044d1f325.xhtml. 

Kim, Donghwa, Deokseong Seo, Suhyoun Cho, and Pilsung Kang. 2019. "Multi-Co-Training for 
Document Classification Using Various Document Representations: TF–IDF, LDA, And 
doc2vec." Information Sciences 477 (March 2019): 15–29. 
https://doi.org/10.1016/j.ins.2018.10.006.

Thomas, Alex. 2020. “Topic Modeling.” In Natural Language Processing with Spark Nlp: 
Learning to Understand Text at Scale. O'Reilly. Sebastopol, CA: O'Reilly Media. 
https://learning.oreilly.com/library/view/natural-languageprocessing/9781492047759/ch10.html#idm45898754698600.

Turney, P. D., and P. Pantel. 2010. “From Frequency to Meaning: Vector Space Models of 
Semantics.” Journal of Artificial Intelligence Research 37 (2010): 141–88. 
https://doi.org/10.1613/jair.2934.
